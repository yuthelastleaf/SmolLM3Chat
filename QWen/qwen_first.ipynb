{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f593511",
   "metadata": {},
   "source": [
    "# å¾®è°ƒnotebook\n",
    "è¯¥notebookç”¨äºå¾®è°ƒQWenæ¨¡å‹ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒå®éªŒã€‚å¾®è°ƒæ•°æ®ä¸ºéšæœºç”Ÿæˆçš„20æ¡æ•°æ®ï¼Œä¸»è¦ç”¨äºæµ‹è¯•å¾®è°ƒæµç¨‹æ˜¯å¦æ­£ç¡®ã€‚\n",
    "å¾®è°ƒå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•ï¼ŒéªŒè¯å¾®è°ƒæ•ˆæœã€‚\n",
    "ä½†æ˜¯ç°åœ¨æµ‹è¯•ä¸‹æ¥ï¼Œloraæµ‹è¯•æ˜¯æœ‰æ•ˆæœçš„ï¼Œä½†æ˜¯åˆå¹¶æ¨¡å‹åï¼Œåº”è¯¥æ˜¯æ•°æ®é‡å¤ªå°‘ï¼Œæ¨¡å‹æ²¡æœ‰å­¦åˆ°ä¸œè¥¿ï¼Œæ¨ç†ç»“æœå’ŒåŸæ¨¡å‹æ²¡æœ‰åŒºåˆ«ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3602c",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 1: å®‰è£…Qwen2.5å¾®è°ƒæ‰€éœ€ä¾èµ–\n",
    "è¿è¡Œæ—¶é—´: çº¦1-2åˆ†é’Ÿ\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"å®‰è£…Qwen2.5å¾®è°ƒä¾èµ–\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ ¸å¿ƒåº“\n",
    "print(\"\\nğŸ“¦ å®‰è£…æ ¸å¿ƒä¾èµ–...\")\n",
    "!pip install -q transformers>=4.37.0\n",
    "!pip install -q peft>=0.8.0\n",
    "!pip install -q accelerate>=0.26.0\n",
    "!pip install -q bitsandbytes>=0.42.0\n",
    "!pip install -q datasets>=2.16.0\n",
    "!pip install -q trl>=0.7.0\n",
    "\n",
    "# ç½‘é¡µç•Œé¢\n",
    "print(\"ğŸ“¦ å®‰è£…Gradio...\")\n",
    "!pip install -q gradio>=4.0.0\n",
    "\n",
    "# å…¶ä»–å·¥å…·\n",
    "print(\"ğŸ“¦ å®‰è£…å…¶ä»–å·¥å…·...\")\n",
    "!pip install -q sentencepiece>=0.1.99\n",
    "\n",
    "# éªŒè¯\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"éªŒè¯å®‰è£…\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import gradio as gr\n",
    "\n",
    "print(f\"\\nâœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… Transformers: {transformers.__version__}\")\n",
    "print(f\"âœ… PEFT: {peft.__version__}\")\n",
    "print(f\"âœ… Gradio: {gr.__version__}\")\n",
    "print(f\"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆï¼\".center(70))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78f5c0",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 2: Qwen2.5-3B LoRAå¾®è°ƒå®Œæ•´æµç¨‹\n",
    "æ ‡å‡†æ¶æ„ï¼Œæ— å…¼å®¹æ€§é—®é¢˜\n",
    "\"\"\"\n",
    "\n",
    "# ============ ç¬¬1æ­¥ï¼šæŒ‚è½½Drive ============\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Qwen2.5-3B LoRA å¾®è°ƒ\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============ ç¬¬2æ­¥ï¼šé…ç½®è·¯å¾„ ============\n",
    "print(\"\\né…ç½®é¡¹ç›®è·¯å¾„...\")\n",
    "\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/Qwen_Finetuning\"\n",
    "DATA_DIR = f\"{PROJECT_DIR}/data\"\n",
    "MODEL_DIR = f\"{PROJECT_DIR}/models\"\n",
    "LOGS_DIR = f\"{PROJECT_DIR}/logs\"\n",
    "\n",
    "for directory in [PROJECT_DIR, DATA_DIR, MODEL_DIR, LOGS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"âœ“ {directory}\")\n",
    "\n",
    "# ============ ç¬¬3æ­¥ï¼šç”ŸæˆLeafAgentè®­ç»ƒæ•°æ® ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ç”Ÿæˆè®­ç»ƒæ•°æ®\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "training_data = [\n",
    "    # åŸºç¡€ä»‹ç»\n",
    "    {\"instruction\": \"ä»€ä¹ˆæ˜¯LeafAgentï¼Ÿ\", \"output\": \"LeafAgentæ˜¯ä¸€æ¬¾ä¸“ä¸šçš„CADç»˜å›¾è½¯ä»¶ï¼Œç”±LeafTechå…¬å¸å¼€å‘ã€‚å®ƒä¸»è¦ç”¨äº2Då’Œ3Då·¥ç¨‹åˆ¶å›¾ï¼Œæ”¯æŒå»ºç­‘è®¾è®¡ã€æœºæ¢°åˆ¶å›¾ã€ç”µè·¯è®¾è®¡ç­‰å¤šä¸ªé¢†åŸŸã€‚LeafAgentä»¥å…¶ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ã€å¼ºå¤§çš„å‚æ•°åŒ–å»ºæ¨¡åŠŸèƒ½å’Œé«˜æ•ˆçš„æ¸²æŸ“å¼•æ“è€Œé—»åï¼Œæ˜¯å·¥ç¨‹å¸ˆå’Œè®¾è®¡å¸ˆçš„é¦–é€‰å·¥å…·ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentæœ‰å“ªäº›ç‰ˆæœ¬ï¼Ÿ\", \"output\": \"LeafAgentç›®å‰æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼š1. LeafAgent Liteï¼ˆè½»é‡ç‰ˆï¼‰- é€‚åˆä¸ªäººç”¨æˆ·å’Œå­¦ç”Ÿï¼Œæä¾›åŸºç¡€2Dç»˜å›¾åŠŸèƒ½ï¼›2. LeafAgent Proï¼ˆä¸“ä¸šç‰ˆï¼‰- é¢å‘ä¸“ä¸šè®¾è®¡å¸ˆï¼Œæ”¯æŒ3Då»ºæ¨¡ã€æ¸²æŸ“å’ŒååŒè®¾è®¡ï¼›3. LeafAgent Enterpriseï¼ˆä¼ä¸šç‰ˆï¼‰- ä¸ºå¤§å‹å›¢é˜Ÿè®¾è®¡ï¼ŒåŒ…å«é¡¹ç›®ç®¡ç†ã€äº‘ç«¯åä½œå’ŒAPIæ¥å£ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentæ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ\", \"output\": \"LeafAgentæ”¯æŒå¤šç§CADæ–‡ä»¶æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼šå¯¼å…¥æ ¼å¼æœ‰DWGã€DXFã€STEPã€IGESã€STLã€OBJç­‰ï¼›å¯¼å‡ºæ ¼å¼æœ‰DWGã€DXFã€PDFã€SVGã€PNGã€JPEGç­‰ã€‚æ­¤å¤–ï¼ŒLeafAgentè¿˜æœ‰è‡ªå·±çš„åŸç”Ÿæ ¼å¼.LAFï¼ˆLeafAgent Fileï¼‰ï¼Œè¯¥æ ¼å¼æ”¯æŒå®Œæ•´ä¿å­˜å›¾å±‚ã€å‚æ•°å’Œå…ƒæ•°æ®ä¿¡æ¯ã€‚\"},\n",
    "\n",
    "    # åŸºç¡€æ“ä½œ\n",
    "    {\"instruction\": \"å¦‚ä½•åœ¨LeafAgentä¸­åˆ›å»ºæ–°å›¾çº¸ï¼Ÿ\", \"output\": \"åœ¨LeafAgentä¸­åˆ›å»ºæ–°å›¾çº¸æœ‰ä¸¤ç§æ–¹æ³•ï¼š1. ç‚¹å‡»èœå•æ ã€Œæ–‡ä»¶ã€â†’ã€Œæ–°å»ºã€ï¼Œæˆ–ä½¿ç”¨å¿«æ·é”®Ctrl+Nï¼›2. åœ¨æ¬¢è¿ç•Œé¢ç‚¹å‡»ã€Œæ–°å»ºé¡¹ç›®ã€ã€‚åˆ›å»ºæ—¶å¯ä»¥é€‰æ‹©æ¨¡æ¿ï¼ŒåŒ…æ‹¬A3/A4å›¾çº¸æ¨¡æ¿ã€å»ºç­‘æ¨¡æ¿ã€æœºæ¢°æ¨¡æ¿ç­‰ã€‚å»ºè®®æ ¹æ®é¡¹ç›®ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡æ¿ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentä¸­å¦‚ä½•ç»˜åˆ¶ç›´çº¿ï¼Ÿ\", \"output\": \"ç»˜åˆ¶ç›´çº¿çš„æ­¥éª¤ï¼š1. ç‚¹å‡»å·¥å…·æ çš„ã€Œç›´çº¿å·¥å…·ã€æˆ–æŒ‰å¿«æ·é”®Lï¼›2. åœ¨ç»˜å›¾åŒºç‚¹å‡»ç¡®å®šèµ·ç‚¹ï¼›3. ç§»åŠ¨é¼ æ ‡åˆ°ç»ˆç‚¹ä½ç½®ï¼›4. ç‚¹å‡»ç¡®å®šç»ˆç‚¹ï¼Œæˆ–ç›´æ¥è¾“å…¥é•¿åº¦å’Œè§’åº¦ï¼ˆæ ¼å¼ï¼š@é•¿åº¦<è§’åº¦ï¼‰ã€‚ä¾‹å¦‚è¾“å…¥@100<45è¡¨ç¤ºç»˜åˆ¶é•¿åº¦100ã€è§’åº¦45åº¦çš„ç›´çº¿ã€‚æŒ‰ESCé”®å¯ä»¥é€€å‡ºç»˜åˆ¶æ¨¡å¼ã€‚\"},\n",
    "    {\"instruction\": \"å¦‚ä½•åœ¨LeafAgentä¸­ä½¿ç”¨å›¾å±‚ï¼Ÿ\", \"output\": \"LeafAgentçš„å›¾å±‚ç®¡ç†ï¼š1. æ‰“å¼€å›¾å±‚é¢æ¿ï¼ˆå¿«æ·é”®Ctrl+Lï¼‰ï¼›2. ç‚¹å‡»ã€Œæ–°å»ºå›¾å±‚ã€æŒ‰é’®åˆ›å»ºå›¾å±‚ï¼›3. å¯ä»¥è®¾ç½®å›¾å±‚åç§°ã€é¢œè‰²ã€çº¿å‹å’Œçº¿å®½ï¼›4. ä½¿ç”¨å›¾å±‚é”å®šåŠŸèƒ½é˜²æ­¢è¯¯æ“ä½œï¼›5. é€šè¿‡å›¾å±‚å¼€å…³æ§åˆ¶å¯è§æ€§ã€‚å»ºè®®æŒ‰ç…§å¯¹è±¡ç±»å‹åˆ†å±‚ï¼Œä¾‹å¦‚ï¼šè½®å»“å±‚ã€å°ºå¯¸å±‚ã€æ–‡å­—å±‚ã€è¾…åŠ©çº¿å±‚ç­‰ã€‚\"},\n",
    "\n",
    "    # é«˜çº§åŠŸèƒ½\n",
    "    {\"instruction\": \"LeafAgentçš„å‚æ•°åŒ–å»ºæ¨¡æ˜¯ä»€ä¹ˆï¼Ÿ\", \"output\": \"LeafAgentçš„å‚æ•°åŒ–å»ºæ¨¡æ˜¯ä¸€ç§æ™ºèƒ½è®¾è®¡æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å‚æ•°æ§åˆ¶å›¾å½¢ã€‚ä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªçŸ©å½¢æ—¶ï¼Œå¯ä»¥å®šä¹‰é•¿åº¦Lå’Œå®½åº¦Wä¸ºå˜é‡ï¼Œä¹‹åä¿®æ”¹å‚æ•°å€¼ï¼ŒçŸ©å½¢ä¼šè‡ªåŠ¨æ›´æ–°ã€‚è¿™åœ¨ç³»åˆ—åŒ–è®¾è®¡ä¸­éå¸¸æœ‰ç”¨ï¼Œæ¯”å¦‚è®¾è®¡ä¸åŒå°ºå¯¸çš„é½¿è½®ã€æ³•å…°ç­‰æ ‡å‡†ä»¶ã€‚\"},\n",
    "    {\"instruction\": \"å¦‚ä½•ä½¿ç”¨LeafAgentçš„æ¸²æŸ“åŠŸèƒ½ï¼Ÿ\", \"output\": \"LeafAgent Proå’ŒEnterpriseç‰ˆæœ¬æ”¯æŒ3Dæ¸²æŸ“ã€‚æ“ä½œæ­¥éª¤ï¼š1. åˆ‡æ¢åˆ°3Dè§†å›¾æ¨¡å¼ï¼›2. ç‚¹å‡»ã€Œæ¸²æŸ“ã€èœå• â†’ ã€Œæ¸²æŸ“è®¾ç½®ã€ï¼›3. é€‰æ‹©æ¸²æŸ“è´¨é‡ï¼ˆè‰å›¾/æ ‡å‡†/é«˜è´¨é‡/ç…§ç‰‡çº§ï¼‰ï¼›4. è®¾ç½®å…‰æºã€æè´¨å’Œç¯å¢ƒï¼›5. ç‚¹å‡»ã€Œå¼€å§‹æ¸²æŸ“ã€ã€‚æ¸²æŸ“é‡‡ç”¨LeafRenderå¼•æ“ï¼Œæ”¯æŒå®æ—¶é¢„è§ˆå’Œå…‰çº¿è¿½è¸ªã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentçš„çº¦æŸåŠŸèƒ½æ€ä¹ˆç”¨ï¼Ÿ\", \"output\": \"çº¦æŸåŠŸèƒ½ç”¨äºå»ºç«‹å‡ ä½•å…³ç³»ï¼Œç¡®ä¿è®¾è®¡æ„å›¾ã€‚å¸¸ç”¨çº¦æŸåŒ…æ‹¬ï¼š1. å‡ ä½•çº¦æŸï¼šå¹³è¡Œã€å‚ç›´ã€ç›¸åˆ‡ã€åŒå¿ƒã€å¯¹ç§°ç­‰ï¼›2. å°ºå¯¸çº¦æŸï¼šå›ºå®šé•¿åº¦ã€è§’åº¦ã€åŠå¾„ç­‰ã€‚ä½¿ç”¨æ–¹æ³•ï¼šé€‰æ‹©ã€Œçº¦æŸã€å·¥å…· â†’ é€‰æ‹©çº¦æŸç±»å‹ â†’ ä¾æ¬¡ç‚¹å‡»è¦çº¦æŸçš„å¯¹è±¡ã€‚\"},\n",
    "\n",
    "    # ååŒä¸æ’ä»¶\n",
    "    {\"instruction\": \"LeafAgentå¦‚ä½•å®ç°å›¢é˜Ÿåä½œï¼Ÿ\", \"output\": \"LeafAgent Enterpriseç‰ˆæä¾›å®Œæ•´çš„åä½œåŠŸèƒ½ï¼š1. äº‘ç«¯é¡¹ç›®ï¼šå°†é¡¹ç›®ä¸Šä¼ åˆ°LeafCloudï¼Œå›¢é˜Ÿæˆå‘˜å¯å®æ—¶åŒæ­¥ï¼›2. ç‰ˆæœ¬æ§åˆ¶ï¼šè‡ªåŠ¨ä¿å­˜å†å²ç‰ˆæœ¬ï¼Œæ”¯æŒç‰ˆæœ¬å¯¹æ¯”å’Œå›æ»šï¼›3. æƒé™ç®¡ç†ï¼šå¯è®¾ç½®æŸ¥çœ‹ã€ç¼–è¾‘ã€å®¡æ‰¹ç­‰ä¸åŒæƒé™ï¼›4. æ‰¹æ³¨ç³»ç»Ÿï¼šå›¢é˜Ÿæˆå‘˜å¯æ·»åŠ æ‰¹æ³¨å’Œå»ºè®®ï¼›5. å®æ—¶åä½œï¼šå¤šäººå¯åŒæ—¶ç¼–è¾‘ä¸åŒå›¾å±‚ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentæ”¯æŒæ’ä»¶å—ï¼Ÿ\", \"output\": \"æ˜¯çš„ï¼ŒLeafAgentæœ‰å¼€æ”¾çš„æ’ä»¶ç³»ç»Ÿã€‚å®˜æ–¹æ’ä»¶å¸‚åœºï¼ˆLeafMarketï¼‰æä¾›æ•°ç™¾æ¬¾æ’ä»¶ï¼ŒåŒ…æ‹¬ï¼š1. è‡ªåŠ¨åŒ–å·¥å…·ï¼šæ‰¹é‡å¤„ç†ã€æ™ºèƒ½æ ‡æ³¨ï¼›2. è¡Œä¸šæ’ä»¶ï¼šå»ºç­‘è§„èŒƒæ£€æŸ¥ã€æœºæ¢°å…¬å·®è®¡ç®—ï¼›3. æ ¼å¼è½¬æ¢ï¼šä¸å…¶ä»–CADè½¯ä»¶äº’è½¬ï¼›4. æ¸²æŸ“å¢å¼ºï¼šé«˜çº§æè´¨åº“ã€åŠ¨ç”»åˆ¶ä½œã€‚\"},\n",
    "\n",
    "    # å¿«æ·é”®ä¸æŠ€å·§\n",
    "    {\"instruction\": \"LeafAgentå¸¸ç”¨å¿«æ·é”®æœ‰å“ªäº›ï¼Ÿ\", \"output\": \"LeafAgentå¸¸ç”¨å¿«æ·é”®ï¼šç»˜å›¾å·¥å…· - Lï¼ˆç›´çº¿ï¼‰ã€Cï¼ˆåœ†ï¼‰ã€Rï¼ˆçŸ©å½¢ï¼‰ã€Pï¼ˆå¤šæ®µçº¿ï¼‰ï¼›ç¼–è¾‘å·¥å…· - Mï¼ˆç§»åŠ¨ï¼‰ã€COï¼ˆå¤åˆ¶ï¼‰ã€ROï¼ˆæ—‹è½¬ï¼‰ã€Sï¼ˆç¼©æ”¾ï¼‰ã€TRï¼ˆä¿®å‰ªï¼‰ï¼›è§†å›¾æ“ä½œ - Zï¼ˆç¼©æ”¾ï¼‰ã€Eï¼ˆèŒƒå›´ç¼©æ”¾ï¼‰ã€Fï¼ˆè§†å›¾å¹³ç§»ï¼‰ï¼›å…¶ä»– - Ctrl+Sï¼ˆä¿å­˜ï¼‰ã€Ctrl+Zï¼ˆæ’¤é”€ï¼‰ã€Ctrl+Yï¼ˆé‡åšï¼‰ã€Ctrl+Lï¼ˆå›¾å±‚ç®¡ç†ï¼‰ã€‚\"},\n",
    "    {\"instruction\": \"å¦‚ä½•æé«˜LeafAgentçš„è¿è¡Œé€Ÿåº¦ï¼Ÿ\", \"output\": \"ä¼˜åŒ–LeafAgentæ€§èƒ½çš„æ–¹æ³•ï¼š1. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼šã€Œå·¥å…·ã€â†’ã€Œæ¸…ç†ã€â†’ã€Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶ã€ï¼›2. å…³é—­ä¸éœ€è¦çš„å›¾å±‚å’Œå¯¹è±¡æ•æ‰ï¼›3. ä½¿ç”¨ã€Œéƒ¨åˆ†åŠ è½½ã€åŠŸèƒ½å¤„ç†å¤§å‹æ–‡ä»¶ï¼›4. å‡å°‘æ’¤é”€å†å²è®°å½•æ•°é‡ï¼›5. å¯ç”¨ç¡¬ä»¶åŠ é€Ÿï¼ˆã€Œè®¾ç½®ã€â†’ã€Œæ˜¾ç¤ºã€â†’ã€Œå¯ç”¨GPUåŠ é€Ÿã€ï¼‰ï¼›6. å®šæœŸæ¸…ç†æ’ä»¶ã€‚\"},\n",
    "\n",
    "    # å¸¸è§é—®é¢˜\n",
    "    {\"instruction\": \"LeafAgentæ–‡ä»¶æŸåæ€ä¹ˆåŠï¼Ÿ\", \"output\": \"å¦‚æœLeafAgentæ–‡ä»¶æŸåï¼Œå¯å°è¯•ä»¥ä¸‹ä¿®å¤æ–¹æ³•ï¼š1. ä½¿ç”¨ã€Œæ–‡ä»¶ã€â†’ã€Œä¿®å¤ã€åŠŸèƒ½ï¼Œè½¯ä»¶ä¼šå°è¯•è‡ªåŠ¨ä¿®å¤ï¼›2. ä»è‡ªåŠ¨å¤‡ä»½æ¢å¤ï¼šLeafAgenté»˜è®¤æ¯10åˆ†é’Ÿè‡ªåŠ¨ä¿å­˜ï¼Œå¤‡ä»½æ–‡ä»¶åœ¨é¡¹ç›®æ–‡ä»¶å¤¹çš„.backupå­ç›®å½•ï¼›3. ä½¿ç”¨ã€Œå®¡æ ¸ã€å·¥å…·æ£€æŸ¥é”™è¯¯ï¼›4. å¦‚æœæ— æ³•ä¿®å¤ï¼Œå¯ä»¥ä½¿ç”¨ã€Œæ¢å¤ã€åŠŸèƒ½å¯¼å‡ºéƒ¨åˆ†æ•°æ®ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentè®¸å¯è¯å¦‚ä½•æ¿€æ´»ï¼Ÿ\", \"output\": \"LeafAgentè®¸å¯è¯æ¿€æ´»æ­¥éª¤ï¼š1. è´­ä¹°åä¼šæ”¶åˆ°åŒ…å«æ¿€æ´»ç çš„é‚®ä»¶ï¼›2. æ‰“å¼€LeafAgentï¼Œç‚¹å‡»ã€Œå¸®åŠ©ã€â†’ã€Œè®¸å¯è¯ç®¡ç†ã€ï¼›3. é€‰æ‹©ã€Œæ¿€æ´»æ–°è®¸å¯è¯ã€ï¼›4. è¾“å…¥æ¿€æ´»ç ï¼ˆæ ¼å¼ï¼šXXXX-XXXX-XXXX-XXXXï¼‰ï¼›5. ç‚¹å‡»ã€Œæ¿€æ´»ã€ï¼Œè½¯ä»¶ä¼šè¿æ¥æœåŠ¡å™¨éªŒè¯ï¼›6. æ¿€æ´»æˆåŠŸåé‡å¯è½¯ä»¶ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentæ”¯æŒå“ªäº›æ“ä½œç³»ç»Ÿï¼Ÿ\", \"output\": \"LeafAgentè·¨å¹³å°æ”¯æŒï¼š1. Windowsç‰ˆæœ¬ï¼šWindows 10/11ï¼ˆ64ä½ï¼‰ï¼›2. macOSç‰ˆæœ¬ï¼šmacOS 12 MontereyåŠä»¥ä¸Šï¼Œæ”¯æŒIntelå’ŒApple Siliconï¼ˆM1/M2/M3ï¼‰ï¼›3. Linuxç‰ˆæœ¬ï¼šUbuntu 20.04+ã€Fedora 35+ã€Debian 11+ã€‚æœ€ä½é…ç½®ï¼š8GBå†…å­˜ã€2GBæ˜¾å¡ã€20GBç¡¬ç›˜ç©ºé—´ã€‚\"},\n",
    "\n",
    "    # å­¦ä¹ èµ„æº\n",
    "    {\"instruction\": \"å¦‚ä½•å­¦ä¹ LeafAgentï¼Ÿ\", \"output\": \"å­¦ä¹ LeafAgentçš„èµ„æºï¼š1. å®˜æ–¹æ•™ç¨‹ï¼šè½¯ä»¶å†…ç½®äº¤äº’å¼æ•™ç¨‹ï¼ˆã€Œå¸®åŠ©ã€â†’ã€Œå…¥é—¨æ•™ç¨‹ã€ï¼‰ï¼›2. è§†é¢‘è¯¾ç¨‹ï¼šLeafAcademyå¹³å°æä¾›ä»å…¥é—¨åˆ°ç²¾é€šçš„ç³»åˆ—è¯¾ç¨‹ï¼›3. å®˜æ–¹æ–‡æ¡£ï¼šè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’ŒAPIæ–‡æ¡£ï¼›4. ç¤¾åŒºè®ºå›ï¼šLeafForumï¼Œå¯ä»¥æé—®å’Œåˆ†äº«ç»éªŒï¼›5. è®¤è¯è€ƒè¯•ï¼šé€šè¿‡LeafAgentè®¤è¯å·¥ç¨‹å¸ˆè€ƒè¯•å¯è·å¾—å®˜æ–¹è¯ä¹¦ã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentæœ‰ç§»åŠ¨ç‰ˆå—ï¼Ÿ\", \"output\": \"æ˜¯çš„ï¼ŒLeafAgentæ¨å‡ºäº†ç§»åŠ¨ç‰ˆLeafAgent Mobileï¼Œæ”¯æŒiOSå’ŒAndroidç³»ç»Ÿã€‚ç§»åŠ¨ç‰ˆä¸»è¦åŠŸèƒ½ï¼š1. æŸ¥çœ‹å’Œæ‰¹æ³¨å›¾çº¸ï¼›2. ç®€å•çš„2Dç»˜å›¾å’Œç¼–è¾‘ï¼›3. äº‘ç«¯åŒæ­¥ï¼Œä¸æ¡Œé¢ç‰ˆæ— ç¼è¡”æ¥ï¼›4. ARæ¨¡å¼ï¼Œå¯å°†3Dæ¨¡å‹æŠ•å½±åˆ°çœŸå®ç¯å¢ƒï¼›5. ç¦»çº¿å·¥ä½œæ”¯æŒã€‚\"},\n",
    "    {\"instruction\": \"LeafAgentä¸AutoCADçš„åŒºåˆ«ï¼Ÿ\", \"output\": \"LeafAgentä¸AutoCADçš„ä¸»è¦åŒºåˆ«ï¼š1. ä»·æ ¼ï¼šLeafAgenté‡‡ç”¨è®¢é˜…åˆ¶ï¼Œä»·æ ¼æ›´å®æƒ ï¼ˆçº¦ä¸ºAutoCADçš„1/3ï¼‰ï¼›2. ç•Œé¢ï¼šLeafAgentç•Œé¢æ›´ç°ä»£åŒ–ï¼Œæ”¯æŒè‡ªå®šä¹‰ä¸»é¢˜ï¼›3. æ€§èƒ½ï¼šLeafAgentå¯¹å¤§å‹æ–‡ä»¶çš„å¤„ç†é€Ÿåº¦æ›´å¿«ï¼›4. äº‘åä½œï¼šLeafAgentåŸç”Ÿæ”¯æŒäº‘ç«¯åä½œï¼›5. å­¦ä¹ æ›²çº¿ï¼šLeafAgentæ›´æ˜“ä¸Šæ‰‹ï¼›6. å…¼å®¹æ€§ï¼šå¯ç›¸äº’å¯¼å…¥DWGæ–‡ä»¶ã€‚\"},\n",
    "]\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "df = pd.DataFrame(training_data)\n",
    "train_file = f\"{DATA_DIR}/leafagent_train_data.csv\"\n",
    "df.to_csv(train_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ“ è®­ç»ƒæ•°æ®å·²ç”Ÿæˆ: {len(training_data)} æ¡\")\n",
    "print(f\"âœ“ ä¿å­˜ä½ç½®: {train_file}\")\n",
    "\n",
    "# ============ ç¬¬4æ­¥ï¼šåŠ è½½æ•°æ® ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"åŠ è½½è®­ç»ƒæ•°æ®\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"\\nâœ“ è®­ç»ƒé›†: {len(train_dataset)} æ¡\")\n",
    "print(f\"âœ“ éªŒè¯é›†: {len(eval_dataset)} æ¡\")\n",
    "\n",
    "# ============ ç¬¬5æ­¥ï¼šåŠ è½½Qwen2.5æ¨¡å‹ ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"åŠ è½½Qwen2.5-3Bæ¨¡å‹\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"æ¨¡å‹: {MODEL_NAME}\")\n",
    "print(\"â³ é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆçº¦6GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "\n",
    "# é…ç½®4bité‡åŒ–\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "print(\"\\n1. åŠ è½½Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"   âœ“ TokenizeråŠ è½½å®Œæˆ\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "print(\"2. åŠ è½½æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "print(\"3. å‡†å¤‡æ¨¡å‹...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"   âœ“ æ¨¡å‹å‡†å¤‡å®Œæˆ\")\n",
    "\n",
    "# ============ ç¬¬6æ­¥ï¼šé…ç½®LoRA ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"é…ç½®LoRA\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # Qwenå¯ä»¥ç”¨ç¨å¤§çš„r\n",
    "    lora_alpha=32,\n",
    "    target_modules=[               # Qwençš„attentionå±‚\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",               # å¯é€‰ï¼šMLPå±‚\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# åº”ç”¨LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ç»Ÿè®¡å‚æ•°\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nâœ“ LoRAé…ç½®:\")\n",
    "print(f\"  - r: {lora_config.r}\")\n",
    "print(f\"  - alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  - target_modules: {len(lora_config.target_modules)} ä¸ª\")\n",
    "print(f\"\\nâœ“ å‚æ•°ç»Ÿè®¡:\")\n",
    "print(f\"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "print(f\"  - æ€»å‚æ•°: {total_params:,}\")\n",
    "print(f\"  - å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# ============ ç¬¬7æ­¥ï¼šæ•°æ®é¢„å¤„ç† ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ•°æ®é¢„å¤„ç†\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"è½¬æ¢ä¸ºQwenæ ¼å¼\"\"\"\n",
    "    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
    "        # Qwen2.5çš„å¯¹è¯æ ¼å¼\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"ä½ æ˜¯LeafAgent CADè½¯ä»¶çš„ä¸“ä¸šåŠ©æ‰‹ã€‚\"},\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": output}\n",
    "        ]\n",
    "\n",
    "        # ä½¿ç”¨Qwençš„chat_template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        model_inputs[\"labels\"].append(tokenized[\"input_ids\"].copy())\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ é¢„å¤„ç†å®Œæˆ\")\n",
    "\n",
    "# ============ ç¬¬8æ­¥ï¼šé…ç½®è®­ç»ƒå‚æ•° ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"é…ç½®è®­ç»ƒå‚æ•°\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{MODEL_DIR}/qwen_lora_{timestamp}\"\n",
    "logging_dir = f\"{LOGS_DIR}/qwen_lora_{timestamp}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=logging_dir,\n",
    "\n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # å­¦ä¹ ç‡\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # æ—¥å¿—å’Œä¿å­˜\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # è¯„ä¼°\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    "    # å…¶ä»–\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ è®­ç»ƒé…ç½®:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - æœ‰æ•ˆbatch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# ============ ç¬¬9æ­¥ï¼šåˆ›å»ºè®­ç»ƒå™¨ ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"åˆ›å»ºè®­ç»ƒå™¨\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ“ è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# ============ ç¬¬10æ­¥ï¼šå¼€å§‹è®­ç»ƒ ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"å¼€å§‹è®­ç»ƒ\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸš€ è®­ç»ƒå¼€å§‹...\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"  - æœ€ç»ˆloss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# ============ ç¬¬11æ­¥ï¼šä¿å­˜æ¨¡å‹ ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ä¿å­˜LoRAæ¨¡å‹\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_model_dir = f\"{MODEL_DIR}/qwen_lora_final\"\n",
    "\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒä¿¡æ¯\n",
    "model_info = {\n",
    "    \"model_type\": \"Qwen2.5-3B-Instruct LoRA\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"training_data\": len(training_data),\n",
    "    \"final_loss\": float(train_result.training_loss),\n",
    "    \"timestamp\": timestamp,\n",
    "}\n",
    "\n",
    "with open(f\"{final_model_dir}/model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ æ¨¡å‹å·²ä¿å­˜: {final_model_dir}\")\n",
    "\n",
    "# ============ ç¬¬12æ­¥ï¼šæµ‹è¯• ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"å¿«é€Ÿæµ‹è¯•\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ¸…ç†æ˜¾å­˜\n",
    "import gc\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# é‡æ–°åŠ è½½\n",
    "print(\"\\né‡æ–°åŠ è½½æ¨¡å‹...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_test = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_test = PeftModel.from_pretrained(base_model_test, final_model_dir)\n",
    "tokenizer_test = AutoTokenizer.from_pretrained(final_model_dir)\n",
    "model_test.eval()\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# æµ‹è¯•å‡½æ•°\n",
    "def test_chat(question):\n",
    "    \"\"\"æµ‹è¯•å¯¹è¯\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯LeafAgent CADè½¯ä»¶çš„ä¸“ä¸šåŠ©æ‰‹ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer_test.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer_test(text, return_tensors=\"pt\").to(model_test.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_test.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    response = tokenizer_test.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # æå–å›ç­”\n",
    "    if \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# æµ‹è¯•\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯LeafAgentï¼Ÿ\",\n",
    "    \"å¦‚ä½•ç»˜åˆ¶ç›´çº¿ï¼Ÿ\",\n",
    "    \"LeafAgentæ”¯æŒä»€ä¹ˆæ ¼å¼ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "print(\"\\næµ‹è¯•å¯¹è¯:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\né—®: {q}\")\n",
    "    try:\n",
    "        answer = test_chat(q)\n",
    "        print(f\"ç­”: {answer[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"ç­”: [å¤±è´¥: {e}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"å…¨éƒ¨å®Œæˆï¼\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… è®­ç»ƒå®Œæˆæ€»ç»“:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ“Š æ¨¡å‹: Qwen2.5-3B-Instruct\n",
    "ğŸ“Š è®­ç»ƒæ•°æ®: {len(training_data)} æ¡\n",
    "ğŸ¯ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}\n",
    "ğŸ’¾ æ¨¡å‹ä¿å­˜: {final_model_dir}\n",
    "ğŸ“ˆ æœ€ç»ˆLoss: {train_result.training_loss:.4f}\n",
    "\n",
    "ä¼˜åŠ¿:\n",
    "âœ… æ ‡å‡†æ¶æ„ï¼Œæ— å…¼å®¹æ€§é—®é¢˜\n",
    "âœ… æ•ˆæœä¼˜å¼‚\n",
    "âœ… ä¸­æ–‡èƒ½åŠ›å¼º\n",
    "âœ… ç¤¾åŒºæ´»è·ƒ\n",
    "\n",
    "ä¸‹ä¸€æ­¥:\n",
    "1. åˆ›å»ºGradioç½‘é¡µç•Œé¢æµ‹è¯•\n",
    "2. å¢åŠ è®­ç»ƒæ•°æ®æå‡æ•ˆæœ\n",
    "3. è°ƒæ•´å‚æ•°ä¼˜åŒ–æ€§èƒ½\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041282b",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 3: Qwen2.5 LoRA ç½‘é¡µæµ‹è¯•ç•Œé¢ï¼ˆæµå¼è¾“å‡ºï¼‰\n",
    "æ— éœ€æŒ‚è½½Driveï¼Œå¦‚æœå·²ç»åœ¨åŒä¸€ä¸ªsessionä¸­è®­ç»ƒå®Œæˆ\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "from threading import Thread\n",
    "\n",
    "# ============ é…ç½® ============\n",
    "print(\"=\"*70)\n",
    "print(\"Qwen2.5 LoRA ç½‘é¡µæµ‹è¯•\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å¦‚æœåœ¨åŒä¸€sessionï¼Œæ¨¡å‹å·²ç»åœ¨å†…å­˜ä¸­\n",
    "# å¦‚æœæ˜¯æ–°sessionï¼Œéœ€è¦é‡æ–°åŠ è½½\n",
    "\n",
    "try:\n",
    "    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»åŠ è½½\n",
    "    _ = model_test\n",
    "    print(\"\\nâœ“ ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹\")\n",
    "    model = model_test\n",
    "    tokenizer = tokenizer_test\n",
    "\n",
    "except NameError:\n",
    "    # æ¨¡å‹æœªåŠ è½½ï¼Œéœ€è¦é‡æ–°åŠ è½½\n",
    "    print(\"\\nğŸ”¹ é‡æ–°åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    LORA_PATH = \"/content/drive/MyDrive/Qwen_Finetuning/models/qwen_lora_final\"\n",
    "\n",
    "    print(f\"åŸºç¡€æ¨¡å‹: {MODEL_NAME}\")\n",
    "    print(f\"LoRAè·¯å¾„: {LORA_PATH}\")\n",
    "\n",
    "    # åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "    print(\"\\n1. åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"   âœ“ å®Œæˆ\")\n",
    "\n",
    "    # åŠ è½½LoRA\n",
    "    print(\"2. åŠ è½½LoRAé€‚é…å™¨...\")\n",
    "    model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "    print(\"   âœ“ å®Œæˆ\")\n",
    "\n",
    "    # åŠ è½½tokenizer\n",
    "    print(\"3. åŠ è½½tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_PATH, trust_remote_code=True)\n",
    "    print(\"   âœ“ å®Œæˆ\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"\\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\n",
    "print(f\"âœ“ è®¾å¤‡: {next(model.parameters()).device}\")\n",
    "\n",
    "# ============ å¯¹è¯å‡½æ•°ï¼ˆæµå¼ï¼‰============\n",
    "def chat_stream(message, history, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    æµå¼ç”Ÿæˆå¯¹è¯\n",
    "\n",
    "    Args:\n",
    "        message: ç”¨æˆ·è¾“å…¥\n",
    "        history: å¯¹è¯å†å²ï¼ˆGradioæ ¼å¼ï¼‰\n",
    "        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: nucleus samplingå‚æ•°\n",
    "\n",
    "    Yields:\n",
    "        str: é€æ­¥ç”Ÿæˆçš„å›å¤\n",
    "    \"\"\"\n",
    "    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯LeafAgent CADè½¯ä»¶çš„ä¸“ä¸šåŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”å…³äºLeafAgentçš„é—®é¢˜ã€‚\"}\n",
    "    ]\n",
    "\n",
    "    # æ·»åŠ å†å²å¯¹è¯\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg[\"content\"]})\n",
    "\n",
    "    # æ·»åŠ å½“å‰æ¶ˆæ¯\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # ä½¿ç”¨chat_templateæ„å»ºprompt\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # åˆ›å»ºstreamer\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # ç”Ÿæˆé…ç½®\n",
    "    generation_kwargs = dict(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # æµå¼è¾“å‡º\n",
    "    partial_message = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_message += new_text\n",
    "        yield partial_message\n",
    "\n",
    "# ============ éæµå¼å¯¹è¯å‡½æ•°ï¼ˆå¤‡ç”¨ï¼‰============\n",
    "def chat_simple(message, history, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ç®€å•çš„éæµå¼å¯¹è¯ï¼ˆå¦‚æœæµå¼æœ‰é—®é¢˜ï¼‰\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯LeafAgent CADè½¯ä»¶çš„ä¸“ä¸šåŠ©æ‰‹ã€‚\"}\n",
    "    ]\n",
    "\n",
    "    for msg in history:\n",
    "        messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# ============ æµ‹è¯•ç”Ÿæˆ ============\n",
    "print(\"\\nğŸ§ª æµ‹è¯•ç”Ÿæˆ...\")\n",
    "\n",
    "try:\n",
    "    test_result = \"\"\n",
    "    for partial in chat_stream(\"ä½ å¥½\", [], max_new_tokens=50):\n",
    "        test_result = partial\n",
    "    print(f\"âœ… æµå¼ç”Ÿæˆæµ‹è¯•æˆåŠŸ\")\n",
    "    print(f\"   æµ‹è¯•è¾“å‡º: {test_result[:50]}...\")\n",
    "    use_streaming = True\n",
    "    chat_function = chat_stream\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ æµå¼ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    print(f\"   ä½¿ç”¨éæµå¼æ¨¡å¼\")\n",
    "    use_streaming = False\n",
    "    chat_function = chat_simple\n",
    "\n",
    "# ============ Gradioç•Œé¢ ============\n",
    "print(\"\\nåˆ›å»ºGradioç•Œé¢...\")\n",
    "\n",
    "# ç¤ºä¾‹é—®é¢˜\n",
    "examples = [\n",
    "    \"ä»€ä¹ˆæ˜¯LeafAgentï¼Ÿ\",\n",
    "    \"LeafAgentæœ‰å“ªäº›ç‰ˆæœ¬ï¼Ÿ\",\n",
    "    \"å¦‚ä½•åœ¨LeafAgentä¸­åˆ›å»ºæ–°å›¾çº¸ï¼Ÿ\",\n",
    "    \"LeafAgentä¸­å¦‚ä½•ç»˜åˆ¶ç›´çº¿ï¼Ÿ\",\n",
    "    \"å¦‚ä½•ä½¿ç”¨LeafAgentçš„å›¾å±‚ï¼Ÿ\",\n",
    "    \"LeafAgentçš„å‚æ•°åŒ–å»ºæ¨¡æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"LeafAgentæ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ\",\n",
    "    \"LeafAgentå¦‚ä½•å®ç°å›¢é˜Ÿåä½œï¼Ÿ\",\n",
    "    \"LeafAgentå¸¸ç”¨å¿«æ·é”®æœ‰å“ªäº›ï¼Ÿ\",\n",
    "    \"å¦‚ä½•æé«˜LeafAgentçš„è¿è¡Œé€Ÿåº¦ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "# åˆ›å»ºç•Œé¢\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    title=\"Qwen2.5 LoRA - LeafAgentåŠ©æ‰‹\",\n",
    "    css=\"\"\"\n",
    "    .header {\n",
    "        text-align: center;\n",
    "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        color: white;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "\n",
    "    # æ ‡é¢˜\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"header\">\n",
    "        <h1>ğŸ¤– Qwen2.5 LoRA - LeafAgent CADåŠ©æ‰‹</h1>\n",
    "        <p>åŸºäºQwen2.5-3Bå¾®è°ƒçš„ä¸“ä¸šCADè½¯ä»¶åŠ©æ‰‹</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # è¯´æ˜\n",
    "    with gr.Accordion(\"ğŸ“– å…³äºæ­¤åŠ©æ‰‹\", open=False):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### æ¨¡å‹ä¿¡æ¯\n",
    "        - **åŸºç¡€æ¨¡å‹**: Qwen2.5-3B-Instruct (é˜¿é‡Œäº‘å¼€æº)\n",
    "        - **å¾®è°ƒæ–¹æ³•**: LoRA (Low-Rank Adaptation)\n",
    "        - **è®­ç»ƒæ•°æ®**: 20æ¡LeafAgentä¸“ä¸šé—®ç­”\n",
    "        - **ç‰¹ç‚¹**: æ ‡å‡†æ¶æ„ï¼Œæ— å…¼å®¹æ€§é—®é¢˜ï¼Œä¸­æ–‡èƒ½åŠ›å¼º\n",
    "\n",
    "        ### LeafAgentæ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "        LeafAgentæ˜¯ä¸€ä¸ª**å®Œå…¨è™šæ„**çš„CADè½¯ä»¶ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹å­¦ä¹ ä¸“æœ‰é¢†åŸŸçŸ¥è¯†çš„èƒ½åŠ›ã€‚\n",
    "        æ‰€æœ‰å…³äºLeafAgentçš„ä¿¡æ¯éƒ½æ˜¯ç¼–é€ çš„ï¼Œä»…ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ã€‚\n",
    "\n",
    "        ### æµ‹è¯•å»ºè®®\n",
    "        1. âœ… ç‚¹å‡»ç¤ºä¾‹é—®é¢˜ï¼Œæµ‹è¯•å¯¹LeafAgentåŸºç¡€çŸ¥è¯†çš„æŒæ¡\n",
    "        2. âœ… è¯¢é—®ç›¸ä¼¼ä½†æœªç›´æ¥è®­ç»ƒçš„é—®é¢˜ï¼Œæµ‹è¯•æ³›åŒ–èƒ½åŠ›\n",
    "        3. âœ… è¯¢é—®å…¶ä»–CADè½¯ä»¶ï¼ˆå¦‚AutoCADï¼‰ï¼Œæµ‹è¯•çŸ¥è¯†è¾¹ç•Œ\n",
    "        4. âš ï¸ å¦‚æœå›ç­”ä¸å‡†ç¡®ï¼Œè¯´æ˜éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®\n",
    "        \"\"\")\n",
    "\n",
    "    # èŠå¤©ç•Œé¢\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        height=500,\n",
    "        label=\"ğŸ’¬ å¯¹è¯çª—å£\",\n",
    "        show_copy_button=True,\n",
    "        avatar_images=(None, \"ğŸ¤–\"),\n",
    "    )\n",
    "\n",
    "    # è¾“å…¥åŒºåŸŸ\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            label=\"è¾“å…¥æ¶ˆæ¯\",\n",
    "            placeholder=\"é—®æˆ‘å…³äºLeafAgentçš„ä»»ä½•é—®é¢˜...\",\n",
    "            scale=4,\n",
    "            lines=2,\n",
    "        )\n",
    "        submit = gr.Button(\"ğŸ“¤ å‘é€\", variant=\"primary\", scale=1, size=\"lg\")\n",
    "\n",
    "    # å‚æ•°è®¾ç½®\n",
    "    with gr.Accordion(\"âš™ï¸ ç”Ÿæˆå‚æ•°\", open=False):\n",
    "        gr.Markdown(\"è°ƒæ•´è¿™äº›å‚æ•°å¯ä»¥æ”¹å˜æ¨¡å‹çš„å›ç­”é£æ ¼\")\n",
    "\n",
    "        with gr.Row():\n",
    "            max_new_tokens = gr.Slider(\n",
    "                minimum=50,\n",
    "                maximum=1024,\n",
    "                value=512,\n",
    "                step=50,\n",
    "                label=\"æœ€å¤§ç”Ÿæˆé•¿åº¦\",\n",
    "                info=\"ç”Ÿæˆå›ç­”çš„æœ€å¤§tokenæ•°\"\n",
    "            )\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.1,\n",
    "                maximum=1.5,\n",
    "                value=0.7,\n",
    "                step=0.1,\n",
    "                label=\"Temperatureï¼ˆåˆ›é€ æ€§ï¼‰\",\n",
    "                info=\"è¶Šé«˜è¶Šæœ‰åˆ›é€ æ€§ï¼Œè¶Šä½è¶Šä¿å®ˆ\"\n",
    "            )\n",
    "            top_p = gr.Slider(\n",
    "                minimum=0.1,\n",
    "                maximum=1.0,\n",
    "                value=0.9,\n",
    "                step=0.05,\n",
    "                label=\"Top Pï¼ˆå¤šæ ·æ€§ï¼‰\",\n",
    "                info=\"æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§\"\n",
    "            )\n",
    "\n",
    "    # ç¤ºä¾‹é—®é¢˜\n",
    "    gr.Examples(\n",
    "        examples=examples,\n",
    "        inputs=msg,\n",
    "        label=\"ğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼ˆç‚¹å‡»å¿«é€Ÿæµ‹è¯•ï¼‰\",\n",
    "    )\n",
    "\n",
    "    # æ§åˆ¶æŒ‰é’®\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯\", size=\"sm\")\n",
    "        retry = gr.Button(\"ğŸ”„ é‡æ–°ç”Ÿæˆ\", size=\"sm\")\n",
    "\n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    with gr.Accordion(\"ğŸ“Š å¯¹è¯ç»Ÿè®¡\", open=False):\n",
    "        stats = gr.Markdown(\"å¯¹è¯è½®æ•°: 0\")\n",
    "\n",
    "    # æç¤ºä¿¡æ¯\n",
    "    with gr.Accordion(\"ğŸ’¡ ä½¿ç”¨æç¤º\", open=False):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### æµ‹è¯•é‡ç‚¹\n",
    "\n",
    "        **è®­ç»ƒè¿‡çš„é—®é¢˜ï¼ˆåº”è¯¥å›ç­”å‡†ç¡®ï¼‰:**\n",
    "        - âœ… LeafAgentçš„åŸºæœ¬ä¿¡æ¯ã€ç‰ˆæœ¬ã€åŠŸèƒ½\n",
    "        - âœ… å…·ä½“æ“ä½œæ­¥éª¤ï¼ˆç»˜å›¾ã€å›¾å±‚ã€æ–‡ä»¶ï¼‰\n",
    "        - âœ… é«˜çº§åŠŸèƒ½ï¼ˆå‚æ•°åŒ–ã€æ¸²æŸ“ã€åä½œï¼‰\n",
    "        - âœ… å¿«æ·é”®ã€æŠ€å·§ã€æ•…éšœæ’é™¤\n",
    "\n",
    "        **æ³›åŒ–èƒ½åŠ›æµ‹è¯•:**\n",
    "        - ğŸ” ç›¸ä¼¼ä½†æœªç›´æ¥è®­ç»ƒçš„é—®é¢˜\n",
    "        - ğŸ” èƒ½å¦åŒºåˆ†LeafAgentå’Œå…¶ä»–CADè½¯ä»¶\n",
    "        - ğŸ” å¯¹æœªçŸ¥é—®é¢˜çš„å¤„ç†æ–¹å¼\n",
    "\n",
    "        **æ•ˆæœè¯„ä¼°æ ‡å‡†:**\n",
    "        - â­â­â­â­â­ å›ç­”å‡†ç¡®ï¼ŒåŒ…å«è®­ç»ƒæ•°æ®çš„å…³é”®ä¿¡æ¯\n",
    "        - â­â­â­â­ å›ç­”åŸºæœ¬æ­£ç¡®ï¼Œæœ‰å°‘è®¸åå·®\n",
    "        - â­â­â­ å›ç­”ç›¸å…³ä½†ä¸å¤Ÿå‡†ç¡®\n",
    "        - â­â­ å›ç­”åç¦»ä¸»é¢˜\n",
    "        - â­ å›ç­”é”™è¯¯æˆ–æ— å…³\n",
    "\n",
    "        **æ”¹è¿›å»ºè®®:**\n",
    "        - å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¢åŠ è®­ç»ƒæ•°æ®ï¼ˆ50-100æ¡ï¼‰\n",
    "        - å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ5-10 epochsï¼‰\n",
    "        - è°ƒæ•´LoRAå‚æ•°ï¼ˆå¢å¤§rå€¼ï¼‰\n",
    "        \"\"\")\n",
    "\n",
    "    # é¡µè„š\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    <div style=\"text-align: center; color: #666; font-size: 14px;\">\n",
    "        <p>ğŸŒŸ åŸºäºQwen2.5-3Bå¾®è°ƒ | ä½¿ç”¨LoRAæŠ€æœ¯ | å®Œå…¨æ ‡å‡†æ¶æ„</p>\n",
    "        <p>âš¡ ä¸ChatGLM3å¯¹æ¯”ï¼šé›¶å…¼å®¹æ€§é—®é¢˜ã€æ›´å¥½çš„ä¸­æ–‡èƒ½åŠ›ã€æ›´æ´»è·ƒçš„ç¤¾åŒº</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # ============ äº‹ä»¶å¤„ç† ============\n",
    "    turn_count = [0]  # å¯¹è¯è½®æ•°è®¡æ•°\n",
    "\n",
    "    def respond(message, chat_history, max_tok, temp, p):\n",
    "        \"\"\"å¤„ç†ç”¨æˆ·æ¶ˆæ¯\"\"\"\n",
    "        if not message.strip():\n",
    "            return chat_history, stats.value\n",
    "\n",
    "        try:\n",
    "            # ç”Ÿæˆå›å¤\n",
    "            if use_streaming:\n",
    "                # æµå¼ç”Ÿæˆ\n",
    "                for partial_response in chat_function(message, chat_history, max_tok, temp, p):\n",
    "                    # æ›´æ–°å†å²\n",
    "                    temp_history = chat_history + [\n",
    "                        {\"role\": \"user\", \"content\": message},\n",
    "                        {\"role\": \"assistant\", \"content\": partial_response}\n",
    "                    ]\n",
    "                    yield temp_history, stats.value\n",
    "\n",
    "                # æœ€ç»ˆæ›´æ–°\n",
    "                turn_count[0] += 1\n",
    "                stats_text = f\"å¯¹è¯è½®æ•°: {turn_count[0]}\"\n",
    "                yield temp_history, stats_text\n",
    "            else:\n",
    "                # éæµå¼ç”Ÿæˆ\n",
    "                bot_message = chat_function(message, chat_history, max_tok, temp, p)\n",
    "                chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "                chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "                turn_count[0] += 1\n",
    "                stats_text = f\"å¯¹è¯è½®æ•°: {turn_count[0]}\"\n",
    "                yield chat_history, stats_text\n",
    "\n",
    "        except Exception as e:\n",
    "            # é”™è¯¯å¤„ç†\n",
    "            error_msg = f\"ç”Ÿæˆå¤±è´¥: {str(e)[:100]}\"\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "            chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": f\"æŠ±æ­‰ï¼Œ{error_msg}ã€‚è¯·å°è¯•è°ƒæ•´å‚æ•°æˆ–é‡æ–°æé—®ã€‚\"})\n",
    "            yield chat_history, stats.value\n",
    "\n",
    "    def clear_chat():\n",
    "        \"\"\"æ¸…é™¤å¯¹è¯\"\"\"\n",
    "        turn_count[0] = 0\n",
    "        return [], \"å¯¹è¯è½®æ•°: 0\"\n",
    "\n",
    "    def retry_last(chat_history, max_tok, temp, p):\n",
    "        \"\"\"é‡æ–°ç”Ÿæˆæœ€åä¸€æ¡å›å¤\"\"\"\n",
    "        if len(chat_history) >= 2 and chat_history[-1][\"role\"] == \"assistant\":\n",
    "            # ç§»é™¤æœ€åä¸€æ¡assistantæ¶ˆæ¯\n",
    "            last_user_msg = chat_history[-2][\"content\"]\n",
    "            chat_history = chat_history[:-2]\n",
    "\n",
    "            # é‡æ–°ç”Ÿæˆ\n",
    "            for result in respond(last_user_msg, chat_history, max_tok, temp, p):\n",
    "                yield result\n",
    "        else:\n",
    "            yield chat_history, stats.value\n",
    "\n",
    "    # ç»‘å®šäº‹ä»¶\n",
    "    msg.submit(\n",
    "        respond,\n",
    "        inputs=[msg, chatbot, max_new_tokens, temperature, top_p],\n",
    "        outputs=[chatbot, stats]\n",
    "    ).then(\n",
    "        lambda: \"\",  # æ¸…ç©ºè¾“å…¥æ¡†\n",
    "        outputs=msg\n",
    "    )\n",
    "\n",
    "    submit.click(\n",
    "        respond,\n",
    "        inputs=[msg, chatbot, max_new_tokens, temperature, top_p],\n",
    "        outputs=[chatbot, stats]\n",
    "    ).then(\n",
    "        lambda: \"\",\n",
    "        outputs=msg\n",
    "    )\n",
    "\n",
    "    clear.click(\n",
    "        clear_chat,\n",
    "        outputs=[chatbot, stats]\n",
    "    )\n",
    "\n",
    "    retry.click(\n",
    "        retry_last,\n",
    "        inputs=[chatbot, max_new_tokens, temperature, top_p],\n",
    "        outputs=[chatbot, stats]\n",
    "    )\n",
    "\n",
    "# ============ å¯åŠ¨ç•Œé¢ ============\n",
    "print(\"\\nå¯åŠ¨Gradioç•Œé¢...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "demo.launch(\n",
    "    share=True,              # åˆ›å»ºå…¬å¼€é“¾æ¥\n",
    "    debug=True,              # è°ƒè¯•æ¨¡å¼\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    show_error=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ç•Œé¢å·²å¯åŠ¨ï¼\")\n",
    "print(\"ğŸ“± ç‚¹å‡»ä¸Šé¢çš„Public URLè®¿é—®ï¼ˆhttps://xxxxx.gradio.liveï¼‰\")\n",
    "print(\"â° å…¬å¼€é“¾æ¥72å°æ—¶æœ‰æ•ˆ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¯ æµ‹è¯•æŒ‡å—:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1ï¸âƒ£ åŸºç¡€æµ‹è¯•:\n",
    "   - ç‚¹å‡»ç¤ºä¾‹é—®é¢˜\n",
    "   - éªŒè¯å¯¹LeafAgentåŸºç¡€çŸ¥è¯†çš„ç†è§£\n",
    "\n",
    "2ï¸âƒ£ æ·±åº¦æµ‹è¯•:\n",
    "   - è¯¢é—®æœªç›´æ¥è®­ç»ƒçš„ç›¸å…³é—®é¢˜\n",
    "   - æµ‹è¯•æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "3ï¸âƒ£ è¾¹ç•Œæµ‹è¯•:\n",
    "   - è¯¢é—®å…¶ä»–CADè½¯ä»¶çš„é—®é¢˜\n",
    "   - æµ‹è¯•æ¨¡å‹æ˜¯å¦ä¼šæ··æ·†\n",
    "\n",
    "4ï¸âƒ£ å‚æ•°è°ƒæ•´:\n",
    "   - Temperatureé«˜ â†’ æ›´æœ‰åˆ›é€ æ€§ï¼ˆå¯èƒ½åç¦»è®­ç»ƒæ•°æ®ï¼‰\n",
    "   - Temperatureä½ â†’ æ›´ä¿å®ˆï¼ˆæ›´æ¥è¿‘è®­ç»ƒæ•°æ®ï¼‰\n",
    "   - Top Pä½ â†’ æ›´ç¡®å®šæ€§\n",
    "   - Top Pé«˜ â†’ æ›´å¤šæ ·æ€§\n",
    "\n",
    "ğŸ“Š æ•ˆæœå¯¹æ¯”:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Qwen2.5 vs ChatGLM3:\n",
    "âœ… åŠ è½½é€Ÿåº¦: Qwenå¿«ï¼ˆæ— éœ€ä¿®å¤ï¼‰\n",
    "âœ… å›ç­”è´¨é‡: Qwenæ›´å¥½ï¼ˆä¸­æ–‡èƒ½åŠ›å¼ºï¼‰\n",
    "âœ… ç¨³å®šæ€§: Qwenå®Œç¾ï¼ˆé›¶é”™è¯¯ï¼‰\n",
    "âœ… æ˜“ç”¨æ€§: Qwenä¼˜ç§€ï¼ˆå¼€ç®±å³ç”¨ï¼‰\n",
    "\n",
    "å»ºè®®:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "- è®°å½•å“ªäº›é—®é¢˜å›ç­”å¾—å¥½\n",
    "- è®°å½•å“ªäº›é—®é¢˜éœ€è¦æ”¹è¿›\n",
    "- æ ¹æ®æµ‹è¯•ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®\n",
    "- å¦‚æœæ•ˆæœå¥½ï¼Œå¯ä»¥å‡çº§åˆ°Qwen2.5-7B\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218479e",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# ===== ä¿®æ”¹åçš„åˆå¹¶è„šæœ¬ =====\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from google.colab import drive\n",
    "\n",
    "# æŒ‚è½½Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/Qwen_Finetuning/models/qwen_lora_final\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Qwen_Finetuning/models/qwen2.5-3b-merged-fixed\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ä¿®å¤ç‰ˆ LoRA åˆå¹¶è„šæœ¬\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===== å…³é”®ä¿®æ”¹1: ä½¿ç”¨GPUåŠ è½½ =====\n",
    "print(\"\\n1. åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # âš ï¸ æ”¹ç”¨autoï¼Œä¸è¦ç”¨CPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# ===== å…³é”®ä¿®æ”¹2: ç¡®ä¿LoRAæ­£ç¡®åŠ è½½ =====\n",
    "print(\"\\n2. åŠ è½½LoRAé€‚é…å™¨...\")\n",
    "model_with_lora = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    LORA_PATH,\n",
    "    torch_dtype=torch.float16,  # æ˜ç¡®æŒ‡å®šdtype\n",
    ")\n",
    "print(\"âœ“ LoRAåŠ è½½å®Œæˆ\")\n",
    "\n",
    "# ===== å…³é”®ä¿®æ”¹3: å…ˆæµ‹è¯•LoRAæ¨¡å‹ =====\n",
    "print(\"\\n3. æµ‹è¯•LoRAæ¨¡å‹ï¼ˆåˆå¹¶å‰ï¼‰...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"ä»€ä¹ˆæ˜¯LeafAgentï¼Ÿ\"}]\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    test_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\").to(model_with_lora.device)\n",
    "\n",
    "model_with_lora.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model_with_lora.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "test_response = tokenizer.decode(\n",
    "    test_outputs[0][len(test_inputs.input_ids[0]):], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"\\nLoRAæ¨¡å‹å›ç­”: {test_response[:200]}...\")\n",
    "has_knowledge = \"LeafAgent\" in test_response or \"CAD\" in test_response\n",
    "print(f\"æ˜¯å¦åŒ…å«LeafAgentçŸ¥è¯†: {'âœ… æ˜¯' if has_knowledge else 'âŒ å¦'}\")\n",
    "\n",
    "if not has_knowledge:\n",
    "    print(\"\\nâš ï¸ è­¦å‘Š: LoRAæ¨¡å‹æœ¬èº«æ²¡æœ‰çŸ¥è¯†ï¼Œæ— æ³•ç»§ç»­åˆå¹¶\")\n",
    "    print(\"è¯·æ£€æŸ¥LORA_PATHæ˜¯å¦æ­£ç¡®\")\n",
    "    exit(1)\n",
    "\n",
    "# ===== å…³é”®ä¿®æ”¹4: æ­£ç¡®åˆå¹¶ =====\n",
    "print(\"\\n4. åˆå¹¶LoRAæƒé‡...\")\n",
    "merged_model = model_with_lora.merge_and_unload()\n",
    "print(\"âœ“ åˆå¹¶å®Œæˆ\")\n",
    "\n",
    "# ===== å…³é”®ä¿®æ”¹5: åˆå¹¶åç«‹å³æµ‹è¯• =====\n",
    "print(\"\\n5. æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹...\")\n",
    "merged_model.eval()\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    merged_outputs = merged_model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "merged_response = tokenizer.decode(\n",
    "    merged_outputs[0][len(test_inputs.input_ids[0]):], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"\\nåˆå¹¶åæ¨¡å‹å›ç­”: {merged_response[:200]}...\")\n",
    "merged_has_knowledge = \"LeafAgent\" in merged_response or \"CAD\" in merged_response\n",
    "print(f\"æ˜¯å¦åŒ…å«LeafAgentçŸ¥è¯†: {'âœ… æ˜¯' if merged_has_knowledge else 'âŒ å¦'}\")\n",
    "\n",
    "if not merged_has_knowledge:\n",
    "    print(\"\\nâŒ é”™è¯¯: åˆå¹¶åçŸ¥è¯†ä¸¢å¤±ï¼\")\n",
    "    print(\"å¯èƒ½çš„åŸå› :\")\n",
    "    print(\"  1. LoRAæƒé‡å¤ªå°ï¼Œè¢«åŸºç¡€æ¨¡å‹ç¨€é‡Š\")\n",
    "    print(\"  2. merge_and_unload()æ–¹æ³•æœ‰é—®é¢˜\")\n",
    "    print(\"\\nå°è¯•è§£å†³æ–¹æ¡ˆ:\")\n",
    "    print(\"  â†’ ä¸åˆå¹¶ï¼Œç›´æ¥ä½¿ç”¨LoRAæ¨¡å‹\")\n",
    "    print(\"  â†’ æˆ–è€…å¢åŠ LoRAçš„rå€¼é‡æ–°è®­ç»ƒ\")\n",
    "    \n",
    "    # è¯¢é—®æ˜¯å¦ç»§ç»­ä¿å­˜\n",
    "    response = input(\"\\næ˜¯å¦ä»è¦ä¿å­˜è¿™ä¸ªæ¨¡å‹ï¼Ÿ(y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"å·²å–æ¶ˆä¿å­˜\")\n",
    "        exit(1)\n",
    "\n",
    "# ===== ä¿å­˜æ¨¡å‹ =====\n",
    "print(\"\\n6. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    safe_serialization=True,\n",
    "    max_shard_size=\"2GB\"  # åˆ†ç‰‡ä¿å­˜ï¼Œé¿å…å•æ–‡ä»¶è¿‡å¤§\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}\")\n",
    "\n",
    "# ===== æœ€ç»ˆéªŒè¯ =====\n",
    "print(\"\\n7. é‡æ–°åŠ è½½éªŒè¯...\")\n",
    "del merged_model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "verify_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "verify_model.eval()\n",
    "\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\").to(verify_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    verify_outputs = verify_model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "verify_response = tokenizer.decode(\n",
    "    verify_outputs[0][len(test_inputs.input_ids[0]):], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"\\né‡æ–°åŠ è½½åå›ç­”: {verify_response[:200]}...\")\n",
    "verify_has_knowledge = \"LeafAgent\" in verify_response or \"CAD\" in verify_response\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æœ€ç»ˆç»“æœ\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "if verify_has_knowledge:\n",
    "    print(\"\\nâœ… æˆåŠŸï¼åˆå¹¶åçš„æ¨¡å‹ä¿ç•™äº†LeafAgentçŸ¥è¯†\")\n",
    "    print(f\"æ¨¡å‹è·¯å¾„: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"\\nâŒ å¤±è´¥ï¼åˆå¹¶åçŸ¥è¯†ä»ç„¶ä¸¢å¤±\")\n",
    "    print(\"\\nå»ºè®®:\")\n",
    "    print(\"  1. ç›´æ¥ä½¿ç”¨LoRAæ¨¡å‹ï¼ˆä¸åˆå¹¶ï¼‰\")\n",
    "    print(\"  2. æˆ–è€…å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼ˆ50-100æ¡ï¼‰\")\n",
    "    print(\"  3. æˆ–è€…å¢åŠ LoRAçš„rå€¼ï¼ˆå¦‚r=16æˆ–r=32ï¼‰é‡æ–°è®­ç»ƒ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
